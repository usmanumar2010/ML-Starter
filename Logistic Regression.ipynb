{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn import datasets\n",
    "import pandas as pd\n",
    "from sklearn import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = datasets.load_iris()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_data = pd.DataFrame(data = iris.data[:,0:2], columns = [\"Sepal Length (cm)\", \"Sepal Width (cm)\"])\n",
    "iris_target = pd.DataFrame(data = (iris.target != 0) * 1, columns = [\"Target\"])\n",
    "iris_df = pd.concat([iris_data, iris_target], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = iris.data[:, :2]\n",
    "y = (iris.target != 0) * 1\n",
    "lr = 0.001\n",
    "num_iter = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = iris.data[:, :2]\n",
    "y = (iris.target != 0) * 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression:\n",
    "    \n",
    "    # defining parameters such as learning rate, number ot iterations, whether to include intercept, \n",
    "    # and verbose which says whether to print anything or not like, loss etc.\n",
    "    def __init__(self, learning_rate=0.01, num_iterations=50000, fit_intercept=True, verbose=False):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.num_iterations = num_iterations\n",
    "        self.fit_intercept = fit_intercept\n",
    "        self.verbose = verbose\n",
    "    \n",
    "    # function to define the Incercept value.\n",
    "    def __b_intercept(self, X):\n",
    "        # initially we set it as all 1's\n",
    "        intercept = np.ones((X.shape[0], 1))\n",
    "        # then we concatinate them to the value of X, we don't add we just append them at the end.\n",
    "        return np.concatenate((intercept, X), axis=1)\n",
    "    \n",
    "    def __sigmoid_function(self, z):\n",
    "        # this is our actual sigmoid function which predicts our y_hat\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "    \n",
    "    def __loss(self, y_hat, y):\n",
    "        # this is the loss function which we use to minimize the error of our model\n",
    "        return (-y * np.log(y_hat) - (1 - y) * np.log(1 - y_hat)).mean()\n",
    "    \n",
    "    # this is the function which trains our model.\n",
    "    def fit(self, X, y):\n",
    "        \n",
    "        # as said if we want our intercept term to be added we use fit_intercept=True\n",
    "        if self.fit_intercept:\n",
    "            X = self.__b_intercept(X)\n",
    "        \n",
    "        # weights initialization of our Normal Vector, initially we set it to 0, then we learn it eventually\n",
    "        self.W = np.zeros(X.shape[1])\n",
    "        \n",
    "        # this for loop runs for the number of iterations provided\n",
    "        for i in range(self.num_iterations):\n",
    "            \n",
    "            # this is our W * Xi\n",
    "            z = np.dot(X, self.W)\n",
    "            \n",
    "            # this is where we predict the values of Y based on W and Xi\n",
    "            y_hat = self.__sigmoid_function(z)\n",
    "            \n",
    "            # this is where the gradient is calculated form the error generated by our model\n",
    "            gradient = np.dot(X.T, (y_hat - y)) / y.size\n",
    "            \n",
    "            # this is where we update our values of W, so that we can use the new values for the next iteration\n",
    "            self.W -= self.learning_rate * gradient\n",
    "            \n",
    "            # this is our new W * Xi\n",
    "            z = np.dot(X, self.W)\n",
    "            y_hat = self.__sigmoid_function(z)\n",
    "            \n",
    "            # this is where the loss is calculated\n",
    "            loss = self.__loss(y_hat, y)\n",
    "            \n",
    "            # as mentioned above if we want to print somehting we use verbose, so if verbose=True then our loss get printed\n",
    "            if(self.verbose ==True and i % 10000 == 0):\n",
    "                print(f'loss: {loss} \\t')\n",
    "    \n",
    "    # this is where we predict the probability values based on out generated W values out of all those iterations.\n",
    "    def predict_prob(self, X):\n",
    "        # as said if we want our intercept term to be added we use fit_intercept=True\n",
    "        if self.fit_intercept:\n",
    "            X = self.__b_intercept(X)\n",
    "        \n",
    "        # this is the final prediction that is generated based on the values learned.\n",
    "        return self.__sigmoid_function(np.dot(X, self.W))\n",
    "    \n",
    "    # this is where we predict the actual values 0 or 1 using round. anything less than 0.5 = 0 or more than 0.5 is 1\n",
    "    def predict(self, X):\n",
    "        return self.predict_prob(X).round()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression(learning_rate=0.1, num_iterations=30)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X, y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6666666666666666"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = model.predict(X)\n",
    "(preds == y).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
